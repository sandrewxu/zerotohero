{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises from Makemore Part 1\n",
    "## 1. Build a trigram language model\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "### 1.1 Parse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.a': 0,\n",
       " '.b': 1,\n",
       " '.c': 2,\n",
       " '.d': 3,\n",
       " '.e': 4,\n",
       " '.f': 5,\n",
       " '.g': 6,\n",
       " '.h': 7,\n",
       " '.i': 8,\n",
       " '.j': 9,\n",
       " '.k': 10,\n",
       " '.l': 11,\n",
       " '.m': 12,\n",
       " '.n': 13,\n",
       " '.o': 14,\n",
       " '.p': 15,\n",
       " '.q': 16,\n",
       " '.r': 17,\n",
       " '.s': 18,\n",
       " '.t': 19,\n",
       " '.u': 20,\n",
       " '.v': 21,\n",
       " '.w': 22,\n",
       " '.x': 23,\n",
       " '.y': 24,\n",
       " '.z': 25,\n",
       " 'a.': 26,\n",
       " 'aa': 27,\n",
       " 'ab': 28,\n",
       " 'ac': 29,\n",
       " 'ad': 30,\n",
       " 'ae': 31,\n",
       " 'af': 32,\n",
       " 'ag': 33,\n",
       " 'ah': 34,\n",
       " 'ai': 35,\n",
       " 'aj': 36,\n",
       " 'ak': 37,\n",
       " 'al': 38,\n",
       " 'am': 39,\n",
       " 'an': 40,\n",
       " 'ao': 41,\n",
       " 'ap': 42,\n",
       " 'aq': 43,\n",
       " 'ar': 44,\n",
       " 'as': 45,\n",
       " 'at': 46,\n",
       " 'au': 47,\n",
       " 'av': 48,\n",
       " 'aw': 49,\n",
       " 'ax': 50,\n",
       " 'ay': 51,\n",
       " 'az': 52,\n",
       " 'b.': 53,\n",
       " 'ba': 54,\n",
       " 'bb': 55,\n",
       " 'bc': 56,\n",
       " 'bd': 57,\n",
       " 'be': 58,\n",
       " 'bf': 59,\n",
       " 'bg': 60,\n",
       " 'bh': 61,\n",
       " 'bi': 62,\n",
       " 'bj': 63,\n",
       " 'bk': 64,\n",
       " 'bl': 65,\n",
       " 'bm': 66,\n",
       " 'bn': 67,\n",
       " 'bo': 68,\n",
       " 'bp': 69,\n",
       " 'bq': 70,\n",
       " 'br': 71,\n",
       " 'bs': 72,\n",
       " 'bt': 73,\n",
       " 'bu': 74,\n",
       " 'bv': 75,\n",
       " 'bw': 76,\n",
       " 'bx': 77,\n",
       " 'by': 78,\n",
       " 'bz': 79,\n",
       " 'c.': 80,\n",
       " 'ca': 81,\n",
       " 'cb': 82,\n",
       " 'cc': 83,\n",
       " 'cd': 84,\n",
       " 'ce': 85,\n",
       " 'cf': 86,\n",
       " 'cg': 87,\n",
       " 'ch': 88,\n",
       " 'ci': 89,\n",
       " 'cj': 90,\n",
       " 'ck': 91,\n",
       " 'cl': 92,\n",
       " 'cm': 93,\n",
       " 'cn': 94,\n",
       " 'co': 95,\n",
       " 'cp': 96,\n",
       " 'cq': 97,\n",
       " 'cr': 98,\n",
       " 'cs': 99,\n",
       " 'ct': 100,\n",
       " 'cu': 101,\n",
       " 'cv': 102,\n",
       " 'cw': 103,\n",
       " 'cx': 104,\n",
       " 'cy': 105,\n",
       " 'cz': 106,\n",
       " 'd.': 107,\n",
       " 'da': 108,\n",
       " 'db': 109,\n",
       " 'dc': 110,\n",
       " 'dd': 111,\n",
       " 'de': 112,\n",
       " 'df': 113,\n",
       " 'dg': 114,\n",
       " 'dh': 115,\n",
       " 'di': 116,\n",
       " 'dj': 117,\n",
       " 'dk': 118,\n",
       " 'dl': 119,\n",
       " 'dm': 120,\n",
       " 'dn': 121,\n",
       " 'do': 122,\n",
       " 'dp': 123,\n",
       " 'dq': 124,\n",
       " 'dr': 125,\n",
       " 'ds': 126,\n",
       " 'dt': 127,\n",
       " 'du': 128,\n",
       " 'dv': 129,\n",
       " 'dw': 130,\n",
       " 'dx': 131,\n",
       " 'dy': 132,\n",
       " 'dz': 133,\n",
       " 'e.': 134,\n",
       " 'ea': 135,\n",
       " 'eb': 136,\n",
       " 'ec': 137,\n",
       " 'ed': 138,\n",
       " 'ee': 139,\n",
       " 'ef': 140,\n",
       " 'eg': 141,\n",
       " 'eh': 142,\n",
       " 'ei': 143,\n",
       " 'ej': 144,\n",
       " 'ek': 145,\n",
       " 'el': 146,\n",
       " 'em': 147,\n",
       " 'en': 148,\n",
       " 'eo': 149,\n",
       " 'ep': 150,\n",
       " 'eq': 151,\n",
       " 'er': 152,\n",
       " 'es': 153,\n",
       " 'et': 154,\n",
       " 'eu': 155,\n",
       " 'ev': 156,\n",
       " 'ew': 157,\n",
       " 'ex': 158,\n",
       " 'ey': 159,\n",
       " 'ez': 160,\n",
       " 'f.': 161,\n",
       " 'fa': 162,\n",
       " 'fb': 163,\n",
       " 'fc': 164,\n",
       " 'fd': 165,\n",
       " 'fe': 166,\n",
       " 'ff': 167,\n",
       " 'fg': 168,\n",
       " 'fh': 169,\n",
       " 'fi': 170,\n",
       " 'fj': 171,\n",
       " 'fk': 172,\n",
       " 'fl': 173,\n",
       " 'fm': 174,\n",
       " 'fn': 175,\n",
       " 'fo': 176,\n",
       " 'fp': 177,\n",
       " 'fq': 178,\n",
       " 'fr': 179,\n",
       " 'fs': 180,\n",
       " 'ft': 181,\n",
       " 'fu': 182,\n",
       " 'fv': 183,\n",
       " 'fw': 184,\n",
       " 'fx': 185,\n",
       " 'fy': 186,\n",
       " 'fz': 187,\n",
       " 'g.': 188,\n",
       " 'ga': 189,\n",
       " 'gb': 190,\n",
       " 'gc': 191,\n",
       " 'gd': 192,\n",
       " 'ge': 193,\n",
       " 'gf': 194,\n",
       " 'gg': 195,\n",
       " 'gh': 196,\n",
       " 'gi': 197,\n",
       " 'gj': 198,\n",
       " 'gk': 199,\n",
       " 'gl': 200,\n",
       " 'gm': 201,\n",
       " 'gn': 202,\n",
       " 'go': 203,\n",
       " 'gp': 204,\n",
       " 'gq': 205,\n",
       " 'gr': 206,\n",
       " 'gs': 207,\n",
       " 'gt': 208,\n",
       " 'gu': 209,\n",
       " 'gv': 210,\n",
       " 'gw': 211,\n",
       " 'gx': 212,\n",
       " 'gy': 213,\n",
       " 'gz': 214,\n",
       " 'h.': 215,\n",
       " 'ha': 216,\n",
       " 'hb': 217,\n",
       " 'hc': 218,\n",
       " 'hd': 219,\n",
       " 'he': 220,\n",
       " 'hf': 221,\n",
       " 'hg': 222,\n",
       " 'hh': 223,\n",
       " 'hi': 224,\n",
       " 'hj': 225,\n",
       " 'hk': 226,\n",
       " 'hl': 227,\n",
       " 'hm': 228,\n",
       " 'hn': 229,\n",
       " 'ho': 230,\n",
       " 'hp': 231,\n",
       " 'hq': 232,\n",
       " 'hr': 233,\n",
       " 'hs': 234,\n",
       " 'ht': 235,\n",
       " 'hu': 236,\n",
       " 'hv': 237,\n",
       " 'hw': 238,\n",
       " 'hx': 239,\n",
       " 'hy': 240,\n",
       " 'hz': 241,\n",
       " 'i.': 242,\n",
       " 'ia': 243,\n",
       " 'ib': 244,\n",
       " 'ic': 245,\n",
       " 'id': 246,\n",
       " 'ie': 247,\n",
       " 'if': 248,\n",
       " 'ig': 249,\n",
       " 'ih': 250,\n",
       " 'ii': 251,\n",
       " 'ij': 252,\n",
       " 'ik': 253,\n",
       " 'il': 254,\n",
       " 'im': 255,\n",
       " 'in': 256,\n",
       " 'io': 257,\n",
       " 'ip': 258,\n",
       " 'iq': 259,\n",
       " 'ir': 260,\n",
       " 'is': 261,\n",
       " 'it': 262,\n",
       " 'iu': 263,\n",
       " 'iv': 264,\n",
       " 'iw': 265,\n",
       " 'ix': 266,\n",
       " 'iy': 267,\n",
       " 'iz': 268,\n",
       " 'j.': 269,\n",
       " 'ja': 270,\n",
       " 'jb': 271,\n",
       " 'jc': 272,\n",
       " 'jd': 273,\n",
       " 'je': 274,\n",
       " 'jf': 275,\n",
       " 'jg': 276,\n",
       " 'jh': 277,\n",
       " 'ji': 278,\n",
       " 'jj': 279,\n",
       " 'jk': 280,\n",
       " 'jl': 281,\n",
       " 'jm': 282,\n",
       " 'jn': 283,\n",
       " 'jo': 284,\n",
       " 'jp': 285,\n",
       " 'jq': 286,\n",
       " 'jr': 287,\n",
       " 'js': 288,\n",
       " 'jt': 289,\n",
       " 'ju': 290,\n",
       " 'jv': 291,\n",
       " 'jw': 292,\n",
       " 'jx': 293,\n",
       " 'jy': 294,\n",
       " 'jz': 295,\n",
       " 'k.': 296,\n",
       " 'ka': 297,\n",
       " 'kb': 298,\n",
       " 'kc': 299,\n",
       " 'kd': 300,\n",
       " 'ke': 301,\n",
       " 'kf': 302,\n",
       " 'kg': 303,\n",
       " 'kh': 304,\n",
       " 'ki': 305,\n",
       " 'kj': 306,\n",
       " 'kk': 307,\n",
       " 'kl': 308,\n",
       " 'km': 309,\n",
       " 'kn': 310,\n",
       " 'ko': 311,\n",
       " 'kp': 312,\n",
       " 'kq': 313,\n",
       " 'kr': 314,\n",
       " 'ks': 315,\n",
       " 'kt': 316,\n",
       " 'ku': 317,\n",
       " 'kv': 318,\n",
       " 'kw': 319,\n",
       " 'kx': 320,\n",
       " 'ky': 321,\n",
       " 'kz': 322,\n",
       " 'l.': 323,\n",
       " 'la': 324,\n",
       " 'lb': 325,\n",
       " 'lc': 326,\n",
       " 'ld': 327,\n",
       " 'le': 328,\n",
       " 'lf': 329,\n",
       " 'lg': 330,\n",
       " 'lh': 331,\n",
       " 'li': 332,\n",
       " 'lj': 333,\n",
       " 'lk': 334,\n",
       " 'll': 335,\n",
       " 'lm': 336,\n",
       " 'ln': 337,\n",
       " 'lo': 338,\n",
       " 'lp': 339,\n",
       " 'lq': 340,\n",
       " 'lr': 341,\n",
       " 'ls': 342,\n",
       " 'lt': 343,\n",
       " 'lu': 344,\n",
       " 'lv': 345,\n",
       " 'lw': 346,\n",
       " 'lx': 347,\n",
       " 'ly': 348,\n",
       " 'lz': 349,\n",
       " 'm.': 350,\n",
       " 'ma': 351,\n",
       " 'mb': 352,\n",
       " 'mc': 353,\n",
       " 'md': 354,\n",
       " 'me': 355,\n",
       " 'mf': 356,\n",
       " 'mg': 357,\n",
       " 'mh': 358,\n",
       " 'mi': 359,\n",
       " 'mj': 360,\n",
       " 'mk': 361,\n",
       " 'ml': 362,\n",
       " 'mm': 363,\n",
       " 'mn': 364,\n",
       " 'mo': 365,\n",
       " 'mp': 366,\n",
       " 'mq': 367,\n",
       " 'mr': 368,\n",
       " 'ms': 369,\n",
       " 'mt': 370,\n",
       " 'mu': 371,\n",
       " 'mv': 372,\n",
       " 'mw': 373,\n",
       " 'mx': 374,\n",
       " 'my': 375,\n",
       " 'mz': 376,\n",
       " 'n.': 377,\n",
       " 'na': 378,\n",
       " 'nb': 379,\n",
       " 'nc': 380,\n",
       " 'nd': 381,\n",
       " 'ne': 382,\n",
       " 'nf': 383,\n",
       " 'ng': 384,\n",
       " 'nh': 385,\n",
       " 'ni': 386,\n",
       " 'nj': 387,\n",
       " 'nk': 388,\n",
       " 'nl': 389,\n",
       " 'nm': 390,\n",
       " 'nn': 391,\n",
       " 'no': 392,\n",
       " 'np': 393,\n",
       " 'nq': 394,\n",
       " 'nr': 395,\n",
       " 'ns': 396,\n",
       " 'nt': 397,\n",
       " 'nu': 398,\n",
       " 'nv': 399,\n",
       " 'nw': 400,\n",
       " 'nx': 401,\n",
       " 'ny': 402,\n",
       " 'nz': 403,\n",
       " 'o.': 404,\n",
       " 'oa': 405,\n",
       " 'ob': 406,\n",
       " 'oc': 407,\n",
       " 'od': 408,\n",
       " 'oe': 409,\n",
       " 'of': 410,\n",
       " 'og': 411,\n",
       " 'oh': 412,\n",
       " 'oi': 413,\n",
       " 'oj': 414,\n",
       " 'ok': 415,\n",
       " 'ol': 416,\n",
       " 'om': 417,\n",
       " 'on': 418,\n",
       " 'oo': 419,\n",
       " 'op': 420,\n",
       " 'oq': 421,\n",
       " 'or': 422,\n",
       " 'os': 423,\n",
       " 'ot': 424,\n",
       " 'ou': 425,\n",
       " 'ov': 426,\n",
       " 'ow': 427,\n",
       " 'ox': 428,\n",
       " 'oy': 429,\n",
       " 'oz': 430,\n",
       " 'p.': 431,\n",
       " 'pa': 432,\n",
       " 'pb': 433,\n",
       " 'pc': 434,\n",
       " 'pd': 435,\n",
       " 'pe': 436,\n",
       " 'pf': 437,\n",
       " 'pg': 438,\n",
       " 'ph': 439,\n",
       " 'pi': 440,\n",
       " 'pj': 441,\n",
       " 'pk': 442,\n",
       " 'pl': 443,\n",
       " 'pm': 444,\n",
       " 'pn': 445,\n",
       " 'po': 446,\n",
       " 'pp': 447,\n",
       " 'pq': 448,\n",
       " 'pr': 449,\n",
       " 'ps': 450,\n",
       " 'pt': 451,\n",
       " 'pu': 452,\n",
       " 'pv': 453,\n",
       " 'pw': 454,\n",
       " 'px': 455,\n",
       " 'py': 456,\n",
       " 'pz': 457,\n",
       " 'q.': 458,\n",
       " 'qa': 459,\n",
       " 'qb': 460,\n",
       " 'qc': 461,\n",
       " 'qd': 462,\n",
       " 'qe': 463,\n",
       " 'qf': 464,\n",
       " 'qg': 465,\n",
       " 'qh': 466,\n",
       " 'qi': 467,\n",
       " 'qj': 468,\n",
       " 'qk': 469,\n",
       " 'ql': 470,\n",
       " 'qm': 471,\n",
       " 'qn': 472,\n",
       " 'qo': 473,\n",
       " 'qp': 474,\n",
       " 'qq': 475,\n",
       " 'qr': 476,\n",
       " 'qs': 477,\n",
       " 'qt': 478,\n",
       " 'qu': 479,\n",
       " 'qv': 480,\n",
       " 'qw': 481,\n",
       " 'qx': 482,\n",
       " 'qy': 483,\n",
       " 'qz': 484,\n",
       " 'r.': 485,\n",
       " 'ra': 486,\n",
       " 'rb': 487,\n",
       " 'rc': 488,\n",
       " 'rd': 489,\n",
       " 're': 490,\n",
       " 'rf': 491,\n",
       " 'rg': 492,\n",
       " 'rh': 493,\n",
       " 'ri': 494,\n",
       " 'rj': 495,\n",
       " 'rk': 496,\n",
       " 'rl': 497,\n",
       " 'rm': 498,\n",
       " 'rn': 499,\n",
       " 'ro': 500,\n",
       " 'rp': 501,\n",
       " 'rq': 502,\n",
       " 'rr': 503,\n",
       " 'rs': 504,\n",
       " 'rt': 505,\n",
       " 'ru': 506,\n",
       " 'rv': 507,\n",
       " 'rw': 508,\n",
       " 'rx': 509,\n",
       " 'ry': 510,\n",
       " 'rz': 511,\n",
       " 's.': 512,\n",
       " 'sa': 513,\n",
       " 'sb': 514,\n",
       " 'sc': 515,\n",
       " 'sd': 516,\n",
       " 'se': 517,\n",
       " 'sf': 518,\n",
       " 'sg': 519,\n",
       " 'sh': 520,\n",
       " 'si': 521,\n",
       " 'sj': 522,\n",
       " 'sk': 523,\n",
       " 'sl': 524,\n",
       " 'sm': 525,\n",
       " 'sn': 526,\n",
       " 'so': 527,\n",
       " 'sp': 528,\n",
       " 'sq': 529,\n",
       " 'sr': 530,\n",
       " 'ss': 531,\n",
       " 'st': 532,\n",
       " 'su': 533,\n",
       " 'sv': 534,\n",
       " 'sw': 535,\n",
       " 'sx': 536,\n",
       " 'sy': 537,\n",
       " 'sz': 538,\n",
       " 't.': 539,\n",
       " 'ta': 540,\n",
       " 'tb': 541,\n",
       " 'tc': 542,\n",
       " 'td': 543,\n",
       " 'te': 544,\n",
       " 'tf': 545,\n",
       " 'tg': 546,\n",
       " 'th': 547,\n",
       " 'ti': 548,\n",
       " 'tj': 549,\n",
       " 'tk': 550,\n",
       " 'tl': 551,\n",
       " 'tm': 552,\n",
       " 'tn': 553,\n",
       " 'to': 554,\n",
       " 'tp': 555,\n",
       " 'tq': 556,\n",
       " 'tr': 557,\n",
       " 'ts': 558,\n",
       " 'tt': 559,\n",
       " 'tu': 560,\n",
       " 'tv': 561,\n",
       " 'tw': 562,\n",
       " 'tx': 563,\n",
       " 'ty': 564,\n",
       " 'tz': 565,\n",
       " 'u.': 566,\n",
       " 'ua': 567,\n",
       " 'ub': 568,\n",
       " 'uc': 569,\n",
       " 'ud': 570,\n",
       " 'ue': 571,\n",
       " 'uf': 572,\n",
       " 'ug': 573,\n",
       " 'uh': 574,\n",
       " 'ui': 575,\n",
       " 'uj': 576,\n",
       " 'uk': 577,\n",
       " 'ul': 578,\n",
       " 'um': 579,\n",
       " 'un': 580,\n",
       " 'uo': 581,\n",
       " 'up': 582,\n",
       " 'uq': 583,\n",
       " 'ur': 584,\n",
       " 'us': 585,\n",
       " 'ut': 586,\n",
       " 'uu': 587,\n",
       " 'uv': 588,\n",
       " 'uw': 589,\n",
       " 'ux': 590,\n",
       " 'uy': 591,\n",
       " 'uz': 592,\n",
       " 'v.': 593,\n",
       " 'va': 594,\n",
       " 'vb': 595,\n",
       " 'vc': 596,\n",
       " 'vd': 597,\n",
       " 've': 598,\n",
       " 'vf': 599,\n",
       " 'vg': 600,\n",
       " 'vh': 601,\n",
       " 'vi': 602,\n",
       " 'vj': 603,\n",
       " 'vk': 604,\n",
       " 'vl': 605,\n",
       " 'vm': 606,\n",
       " 'vn': 607,\n",
       " 'vo': 608,\n",
       " 'vp': 609,\n",
       " 'vq': 610,\n",
       " 'vr': 611,\n",
       " 'vs': 612,\n",
       " 'vt': 613,\n",
       " 'vu': 614,\n",
       " 'vv': 615,\n",
       " 'vw': 616,\n",
       " 'vx': 617,\n",
       " 'vy': 618,\n",
       " 'vz': 619,\n",
       " 'w.': 620,\n",
       " 'wa': 621,\n",
       " 'wb': 622,\n",
       " 'wc': 623,\n",
       " 'wd': 624,\n",
       " 'we': 625,\n",
       " 'wf': 626,\n",
       " 'wg': 627,\n",
       " 'wh': 628,\n",
       " 'wi': 629,\n",
       " 'wj': 630,\n",
       " 'wk': 631,\n",
       " 'wl': 632,\n",
       " 'wm': 633,\n",
       " 'wn': 634,\n",
       " 'wo': 635,\n",
       " 'wp': 636,\n",
       " 'wq': 637,\n",
       " 'wr': 638,\n",
       " 'ws': 639,\n",
       " 'wt': 640,\n",
       " 'wu': 641,\n",
       " 'wv': 642,\n",
       " 'ww': 643,\n",
       " 'wx': 644,\n",
       " 'wy': 645,\n",
       " 'wz': 646,\n",
       " 'x.': 647,\n",
       " 'xa': 648,\n",
       " 'xb': 649,\n",
       " 'xc': 650,\n",
       " 'xd': 651,\n",
       " 'xe': 652,\n",
       " 'xf': 653,\n",
       " 'xg': 654,\n",
       " 'xh': 655,\n",
       " 'xi': 656,\n",
       " 'xj': 657,\n",
       " 'xk': 658,\n",
       " 'xl': 659,\n",
       " 'xm': 660,\n",
       " 'xn': 661,\n",
       " 'xo': 662,\n",
       " 'xp': 663,\n",
       " 'xq': 664,\n",
       " 'xr': 665,\n",
       " 'xs': 666,\n",
       " 'xt': 667,\n",
       " 'xu': 668,\n",
       " 'xv': 669,\n",
       " 'xw': 670,\n",
       " 'xx': 671,\n",
       " 'xy': 672,\n",
       " 'xz': 673,\n",
       " 'y.': 674,\n",
       " 'ya': 675,\n",
       " 'yb': 676,\n",
       " 'yc': 677,\n",
       " 'yd': 678,\n",
       " 'ye': 679,\n",
       " 'yf': 680,\n",
       " 'yg': 681,\n",
       " 'yh': 682,\n",
       " 'yi': 683,\n",
       " 'yj': 684,\n",
       " 'yk': 685,\n",
       " 'yl': 686,\n",
       " 'ym': 687,\n",
       " 'yn': 688,\n",
       " 'yo': 689,\n",
       " 'yp': 690,\n",
       " 'yq': 691,\n",
       " 'yr': 692,\n",
       " 'ys': 693,\n",
       " 'yt': 694,\n",
       " 'yu': 695,\n",
       " 'yv': 696,\n",
       " 'yw': 697,\n",
       " 'yx': 698,\n",
       " 'yy': 699,\n",
       " 'yz': 700,\n",
       " 'z.': 701,\n",
       " 'za': 702,\n",
       " 'zb': 703,\n",
       " 'zc': 704,\n",
       " 'zd': 705,\n",
       " 'ze': 706,\n",
       " 'zf': 707,\n",
       " 'zg': 708,\n",
       " 'zh': 709,\n",
       " 'zi': 710,\n",
       " 'zj': 711,\n",
       " 'zk': 712,\n",
       " 'zl': 713,\n",
       " 'zm': 714,\n",
       " 'zn': 715,\n",
       " 'zo': 716,\n",
       " 'zp': 717,\n",
       " 'zq': 718,\n",
       " 'zr': 719,\n",
       " 'zs': 720,\n",
       " 'zt': 721,\n",
       " 'zu': 722,\n",
       " 'zv': 723,\n",
       " 'zw': 724,\n",
       " 'zx': 725,\n",
       " 'zy': 726,\n",
       " 'zz': 727}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = set(\"\".join(words))\n",
    "chars.add('.')\n",
    "chars = sorted(chars)\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "twochars = set()\n",
    "chars.append('.')\n",
    "for a in chars:\n",
    "    for b in chars:\n",
    "        twochars.add(a+b)\n",
    "twochars = sorted(twochars)\n",
    "twochars.pop(0)\n",
    "sstoi = {s:i for i, s, in enumerate(twochars)}\n",
    "itoss = {i:s for s,i in sstoi.items()}\n",
    "\n",
    "sstoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "# Inputs need to be \"aa\", \"bb\", etc. (26*26+26+26)\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append(sstoi[ch1+ch2])\n",
    "        ys.append(stoi[ch3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "for x, y in zip(xs, ys):\n",
    "    x2 = x % 27\n",
    "    x1 = x // 27\n",
    "    print(f\"{itos[x1]} {itos[x2]} -> {itos[y]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: two characters\n",
    "# Output: next character, then continue until the end character\n",
    "# Need a 728 x 27 tensor for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)\n",
    "\n",
    "# Initiliaze the model\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((728, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 20: 2.361903429031372\n",
      "Loss at iteration 40: 2.358445882797241\n",
      "Loss at iteration 60: 2.355074405670166\n",
      "Loss at iteration 80: 2.351785182952881\n",
      "Loss at iteration 100: 2.348576307296753\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(100):\n",
    "\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=728).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        W += -5 * W.grad\n",
    "\n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"Loss at iteration {i+1}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3486, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generate from neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daexze\n",
      "daogjkus\n",
      "dila\n",
      "da\n",
      "dah\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  start = 'd'\n",
    "  out = [start]\n",
    "  ix =  sstoi['.' + start] # start with '.a' - '.z', need to be between 0 and 25\n",
    "  while True:\n",
    "    logits = W[[ix]]\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    next = itos[ix]\n",
    "    if ix == 0:\n",
    "      break\n",
    "    out.append(next)\n",
    "    ix = sstoi[start + next]\n",
    "    start = next\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the dataset\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "random.shuffle(words)\n",
    "numwords = len(words)\n",
    "\n",
    "train = words[:int(numwords*0.8)]\n",
    "val = words[int(numwords*0.8):int(numwords*0.9)]\n",
    "test = words[int(numwords*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 156942\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "# Inputs need to be \"aa\", \"bb\", etc. (26*26+26+26)\n",
    "for w in train:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append(sstoi[ch1+ch2])\n",
    "        ys.append(stoi[ch3])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples:', num)\n",
    "\n",
    "# Initiliaze the model\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((728, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 20: 3.027872323989868\n",
      "Loss at iteration 40: 2.761096954345703\n",
      "Loss at iteration 60: 2.6268115043640137\n",
      "Loss at iteration 80: 2.5431036949157715\n",
      "Loss at iteration 100: 2.485710620880127\n",
      "Loss at iteration 120: 2.4437415599823\n",
      "Loss at iteration 140: 2.411585807800293\n",
      "Loss at iteration 160: 2.3861031532287598\n",
      "Loss at iteration 180: 2.3653981685638428\n",
      "Loss at iteration 200: 2.3482425212860107\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(200):\n",
    "\n",
    "    # Forward pass\n",
    "    logits = W[xs]\n",
    "    loss = F.cross_entropy(logits, ys) + 0.1*(W**2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        W += -50 * W.grad\n",
    "\n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"Loss at iteration {i+1}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss without L2 regularization: 2.3482425212860107\n"
     ]
    }
   ],
   "source": [
    "print('Training loss without L2 regularization:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss with 1 L2: 2.3131976425647736\n"
     ]
    }
   ],
   "source": [
    "print('Training loss with 1 L2:', loss.item() - 1*(W**2).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss with 0.1 L2: 2.2590897500514986\n"
     ]
    }
   ],
   "source": [
    "print('Training loss with 0.1 L2:', loss.item() - 0.1*(W**2).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss with 0.01 L2: 2.2583543026447295\n"
     ]
    }
   ],
   "source": [
    "print('Training loss with 0.01 L2:', loss.item() - 0.01*(W**2).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of validation examples: 19529\n",
      "Validation loss: 2.2621829509735107\n"
     ]
    }
   ],
   "source": [
    "# Validation and testing loss\n",
    "\n",
    "valxs, valys = [], []\n",
    "# Inputs need to be \"aa\", \"bb\", etc. (26*26+26+26)\n",
    "\n",
    "for w in val:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        valxs.append(sstoi[ch1+ch2])\n",
    "        valys.append(stoi[ch3])\n",
    "\n",
    "valxs = torch.tensor(valxs)\n",
    "valys = torch.tensor(valys)\n",
    "\n",
    "valnum = valxs.nelement()\n",
    "print('number of validation examples:', valnum)\n",
    "\n",
    "logits = W[valxs]\n",
    "loss = F.cross_entropy(logits, valys)\n",
    "print(f\"Validation loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test examples: 19642\n",
      "Test loss: 2.2907931804656982\n"
     ]
    }
   ],
   "source": [
    "testxs, testys = [], []\n",
    "\n",
    "for w in test:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        testxs.append(sstoi[ch1+ch2])\n",
    "        testys.append(stoi[ch3])\n",
    "\n",
    "testxs = torch.tensor(testxs)\n",
    "testys = torch.tensor(testys)\n",
    "\n",
    "testnum = testxs.nelement()\n",
    "print('number of test examples:', testnum)\n",
    "\n",
    "logits = W[testxs]\n",
    "loss = F.cross_entropy(logits, testys)\n",
    "print(f\"Test loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val loss was 2.28ish, test loss was 2.29 ish was without L2 regularization\n",
    "1 L2 regularization increased training and validation losses to 2.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smoothing the trigram model\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deleting one-hot\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross entropy instead of NLL\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Meta-exercise\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
