# LLM Crash Course

In the repository are my jupyter notebooks from following Andrej Karpathy's series on Neural Networks, Zero to Hero, from micrograd to GPT-2. Additionally, I also look at recent papers such as Meta AI's "Llama 3 Herd of Models".

## Micrograd
Backpropogation and training an MLP on a very simple model

## Makemore
Character-level language generation.
1. Bigram model with single layer
2. Bag of words (multi-layer perceptron)
3. Activations and gradients
4. Backpropagation
5. WaveNet

## Generative Pre-trained Transformer (GPT)
1. GPT (Building a character-level transformer-based language model on Tiny Shakespeares)
2. GPT Tokenizer
3. GPT-2 (124M)

## Additional
1. State of GPT (Andrej Karpathy, Microsoft Build 2023)
2. The Llama 3 Herd of Models (July 2024)